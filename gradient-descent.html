<!doctype html>
<html class="no-js" lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />

		<title>kamidox.com</title>
		<meta name="description" content="">
		<meta name="author" content="Joey Huang">

		<link rel="stylesheet" href="./theme/css/foundation.css" />
		<link rel="stylesheet" href="./theme/css/pygment/monokai.css" />
		<link rel="stylesheet" href="./theme/css/custom.css" />


		<link rel="shortcut icon" href="./theme/img/favicon.ico">

		<script src="./theme/js/modernizr.js"></script>

		<!-- Feeds -->


		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script>
		<script>
		MathJax.Hub.Config({
		  config: ["MMLorHTML.js"],
		  extensions: ["tex2jax.js"],
		  jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML"],
		  tex2jax: {
		    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		    processEscapes: true
		  },
		  TeX: {
		    extensions: ["AMSmath.js", "AMSsymbols.js"],
		    TagSide: "right",
		    TagIndent: ".8em",
		    MultLineWidth: "85%",
		    equationNumbers: {
		      autoNumber: "AMS",
		    },
		    unicode: {
		      fonts: "STIXGeneral,'Arial Unicode MS'"
		    }
		  },
		  displayAlign: "center",
		  showProcessingMessages: false,
		  messageStyle: 'none'
		});
		</script>
	</head>
	<body>
		<div class="off-canvas-wrap">
			<div class="inner-wrap">
				<!-- mobile top bar to activate nav -->
				<nav class="tab-bar show-for-small">
					<section class="left-small">
						<a class="left-off-canvas-toggle menu-icon" ><span></span></a>
					</section>

					<section class="middle tab-bar-section">
						<h1 class="title">kamidox.com</h1>
					</section>
				</nav>

				<!-- mobile side bar nav -->
				<aside class="left-off-canvas-menu">
					<ul class="off-canvas-list">
							<li><a href="http://blog.kamidox.com">Home</a></li>
							<li><a href="http://blog.kamidox.com/about.html">About</a></li>

						<li><label>Categories</label></li>
							<li ><a href="./category/android.html">android</a></li>
							<li ><a href="./category/essay.html">essay</a></li>
							<li ><a href="./category/flask.html">flask</a></li>
							<li class="active"><a href="./category/ml.html">ml</a></li>
							<li ><a href="./category/nlp.html">nlp</a></li>
							<li ><a href="./category/python.html">python</a></li>
							<li ><a href="./category/tools.html">tools</a></li>
							<li ><a href="./category/weapp.html">weapp</a></li>
							<li ><a href="./category/web.html">web</a></li>
							<li ><a href="./category/werkzeug.html">werkzeug</a></li>




						<li><label>Monthly Archives</label></li>
									<li><a href="/posts/2018/03/index.html">March 2018 (1)</a></li>
									<li><a href="/posts/2017/05/index.html">May 2017 (2)</a></li>
									<li><a href="/posts/2017/04/index.html">April 2017 (1)</a></li>
									<li><a href="/posts/2017/02/index.html">February 2017 (1)</a></li>
									<li><a href="/posts/2017/01/index.html">January 2017 (1)</a></li>
									<li><a href="/posts/2016/12/index.html">December 2016 (2)</a></li>
									<li><a href="/posts/2016/11/index.html">November 2016 (3)</a></li>
									<li><a href="/posts/2016/10/index.html">October 2016 (1)</a></li>
									<li><a href="/posts/2016/09/index.html">September 2016 (1)</a></li>
									<li><a href="/posts/2016/03/index.html">March 2016 (2)</a></li>
									<li><a href="/posts/2016/02/index.html">February 2016 (2)</a></li>
									<li><a href="/posts/2016/01/index.html">January 2016 (2)</a></li>
									<li><a href="/posts/2015/12/index.html">December 2015 (10)</a></li>
									<li><a href="/posts/2015/11/index.html">November 2015 (6)</a></li>
									<li><a href="/posts/2015/10/index.html">October 2015 (2)</a></li>
									<li><a href="/posts/2015/09/index.html">September 2015 (7)</a></li>
									<li><a href="/posts/2015/08/index.html">August 2015 (1)</a></li>
									<li><a href="/posts/2015/07/index.html">July 2015 (1)</a></li>
									<li><a href="/posts/2015/05/index.html">May 2015 (1)</a></li>
									<li><a href="/posts/2015/04/index.html">April 2015 (1)</a></li>
									<li><a href="/posts/2015/03/index.html">March 2015 (3)</a></li>
									<li><a href="/posts/2015/02/index.html">February 2015 (2)</a></li>
									<li><a href="/posts/2015/01/index.html">January 2015 (2)</a></li>
									<li><a href="/posts/2014/12/index.html">December 2014 (3)</a></li>
									<li><a href="/posts/2014/11/index.html">November 2014 (4)</a></li>
									<li><a href="/posts/2014/10/index.html">October 2014 (6)</a></li>
									<li><a href="/posts/2014/09/index.html">September 2014 (1)</a></li>
									<li><a href="/posts/2014/07/index.html">July 2014 (1)</a></li>


					</ul>
				</aside>

				<!-- top bar nav -->
				<nav class="top-bar hide-for-small-only" data-topbar>
					<ul class="title-area">
						<li class="name">
							<h1><a href="./">kamidox.com</a></h1>
						</li>
					</ul>

					<section class="top-bar-section">
						<ul class="left">
								<li><a href="http://blog.kamidox.com">Home</a></li>
								<li><a href="http://blog.kamidox.com/about.html">About</a></li>

						</ul>
					</section>
				</nav>

				<!-- Main Page Content and Sidebar -->
				<section class="main-section">
					<div class="row">
						<!-- Main Content -->
						<div class="medium-9 small-12 columns" role="content">
<article>
	<h2>线性回归算法</h2>
	<p>本文总结了线性回归算法里用到的一些微积分知识，接着根据最小均方差推导出梯度下降算法以及优化后的随机梯度下降算法。</p>
<h2 id="_1">微积分基本运算法则</h2>
<ul>
<li><strong>法则一</strong>：对 $y(x)=cx^n$ ，其针对 x 的偏导数为 $\frac{\partial}{\partial x}f(x)=cnx^{n-1}$</li>
<li><strong>法则二</strong>：常数的微分为 0</li>
<li><strong>法则三</strong>：偏导数可以穿透累加器，即 $$\frac{\partial}{\partial x_0}\sum_{i=0}^nF(x_i) = \sum_{i=0}^n\frac{\partial}{\partial x_0}F(x_i)$$</li>
<li><strong>法则四</strong>：微分链接法则，比如 $f(x)$ 是以 x 为自变量的函数，令 $J(x)=g(f(x))$ ，则 $J(x)$ 的微分方程为 $$\frac{\partial}{\partial x}J(x) = g&rsquo;(f(x))\times f&rsquo;(x)$$</li>
<li><strong>法则五</strong>：计算偏导数时，把求导变量当作变量，其他的变量当作常数，比如对方程 $f(x, y) = ax^n + by^m$，则 $$\frac{\partial}{\partial x}f(x, y) = na x^{n-1}$$ 因为是对 x 求导，所以可以把 y 当成常数，即 $by^m$ 整个算子就是一个常数，根据第二个法则，常数的导数为 0。同理，$$\frac{\partial}{\partial y}f(x, y) = mby^{m-1}$$</li>
</ul>
<p>维基百科上有教程可以参考，比如 <a href="https://en.wikipedia.org/wiki/Chain_rule">Chain Rule</a> 和 <a href="https://en.wikipedia.org/wiki/Partial_derivatives">Partial Derivatives</a>。</p>
<h2 id="_2">线性回归算法</h2>
<p>假设我们<strong>训练数据集 (training data set)</strong> 有 m 个数据 $(x_0, y_0), (x_1, y_1), &hellip; (x_m, y_m)$ ，我们用线性方程 $h(x) = \theta_0 + \theta_1 x$ 来拟合这组数据，怎么样来选取参数 $\theta_0$ 和 $\theta_1$ 来最优拟合这组数据呢？</p>
<p>我们可以把这 m 个点画在二维坐标系里，然后计算这 m 个点到我们的线性方程所描述的直线的最短距离，当这些点到我们的拟合直线的距离总和最小时，那么我们就找到了最优的拟合方案了。所以，问题转化为求下面函数的最小值：</p>
<p>$$<br />
J(\theta) = J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^m(h(x^{(i)}) - y^{(i)})^2<br />
$$</p>
<p>上面的公式叫<strong>成本函数 (Cost Function)</strong>，其中 $h(x_i)$ 是我们的拟合函数针对 $x_i$ 这个点预测出来的值。乘以 $\frac12$ 是为了计算方便，后文我们会看到。</p>
<p>上面我们只考虑了一个变量 $x$ ，即决定这组数据 $y$ 值的只有一个变量。考虑更一般的情况，有 n 个变量 $x_1, x_2, x_3, &hellip; x_n$ 决定 $y$ 的值，那么我们的预测函数模型可以改写如下：</p>
<p>$$<br />
h(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + &hellip; + \theta_n x_n<br />
$$</p>
<p>我们让 $x_0$ 为常数 1，用累加器运算符重写上面的预测函数</p>
<p>$$<br />
h(x) = \sum_{j=0}^n \theta_j x_j<br />
$$</p>
<p>$\theta_0, \theta_1, &hellip; \theta_n$ 我们统称为 $\theta$，是我们的预测函数的 n 个<strong>参数 (parameters)</strong>。即一组 $\theta$ 值就决定了一个预测函数，我们记作 $h_\theta(x)$，为了简便起见，在不引起误解的情况下我们也把它简写为 $h(x)$。理论上，预测函数有无穷多个，我们求解的目标就是找出一个最优的 $\theta$ 值。</p>
<div class="admonition hint">
<p class="admonition-title">考考你</p>
<p>当有 n 个变量 $x_1, x_2, &hellip; x_n$ 决定 y 的值的时候，训练数据集应该长什么样呢？</p>
</div>
<p>为了计算 $J(\theta)$ 的最小值，我们选取一组初始的 $\theta$ ，然后逐步调整 $\theta$ 的值，以便让 $J(\theta)$ 逐渐变小，最后我们希望能让 $J(\theta)$ 收敛在一个极值附近，这样我们就找到了最优或局部最优的解。$\theta$ 的迭代公式为：</p>
<p>$$<br />
\theta_j = \theta_j - \alpha \frac\partial{\partial{\theta_j}}J(\theta)<br />
$$</p>
<p>其中，<strong>$\alpha$</strong> 是叫<strong>学习率 (learning rate)</strong>，表示我们一次要让 $\theta_j$ 往前迈多大步子。如果步子太小，意味着要计算很多次才能到达目的地，如果步子太大，可以会直接跨过目的地，从而无法收敛。$\frac\partial{\partial{\theta_j}}J(\theta)$ 就是成本函数的<strong>偏导数 (partial derivatives)</strong>。</p>
<div class="admonition hint">
<p class="admonition-title">偏导数的物理意义</p>
<p>在这个公式里，可以简单地把偏导数理解为斜率。我们要让 $\theta_j$ 不停地迭代，则根据当前 $\theta_j$ 的值，我们算出 $J(\theta)$ 在 $\theta_j$ 上的斜率，然后再乘以我们的学习率 $\alpha$ 就让我们的 $\theta_j$ 往前迈了一小步。</p>
</div>
<p>现在问题转化为求 $J(\theta)$ 的偏导数，这个推导过程会用到文章开头部分介绍的几个微积分运算基本法则。</p>
<p>根据成本函数的定义，以及文章开头的几个微积分基本运算法则，我们可以求解参数迭代公式里偏微分算子。</p>
<p>$$<br />
\begin{align}<br />
\frac\partial{\partial{\theta_j}}J(\theta) &amp; = \frac\partial{\partial{\theta_j}} \frac{1}{2m}\sum_{i=1}^m(h(x^{(i)}) - y^{(i)})^2 \\<br />
&amp; = \frac{1}{2m}\sum_{i=1}^m \frac\partial{\partial{\theta_j}} (h(x^{(i)}) - y^{(i)})^2 \\<br />
&amp; = 2 \frac{1}{2m} \sum_{i=1}^m \left((h(x^{(i)}) - y^{(i)}) \frac\partial{\partial{\theta_j}} \left(h(x^{(i)}) - y^{(i)}\right)\right) \\<br />
&amp; = \frac{1}{m} \sum_{i=1}^m \left(\left(h(x^{(i)}) - y^{(i)}\right) \frac\partial{\partial{\theta_j}} \left(\sum_{j=0}^n \theta_j x_j^{(i)} - y^{(i)}\right)\right) \\<br />
&amp; = \frac{1}{m} \sum_{i=1}^m \left(\left(h(x^{(i)}) - y^{(i)}\right) x_j^{(i)}\right) \\<br />
\end{align}<br />
$$</p>
<p>式子 (2) 是根据上文的法则三得到的。式子 (3) 是根据上文的法则四得到的，这里也可以看到之前除以 2 的目的是为了抵消计算偏导数时乘以 2。式子 (5) 是根据上文的法则五得到的。</p>
<p>最后得出我们的参数迭代函数</p>
<p>$$<br />
\begin{align}<br />
\theta_j &amp; = \theta_j - \frac{\alpha}{m} \sum_{i=1}^m \left(\left(h(x^{(i)}) - y^{(i)}\right) x_j^{(i)}\right)<br />
\end{align}<br />
$$</p>
<p>这个就是 <strong>LSM</strong> (Least Mean Squares) 迭代算法，也叫 <strong>Widrow-Hoff</strong> 学习算法。</p>
<p>解析一下这个公式几个关键部分的含义</p>
<ul>
<li>$h(x^{(i)})$: 这个是按照我们的给定的参数的预测值，只要 $\theta$ 确定了，我们就可以根据预测函数算出这个值</li>
<li>$y^{(i)}$: 这个是<strong>训练数据集 (training data set)</strong> 的目标值</li>
<li>$x_j^{(i)}$: 这个是训练数据集里第 j 个变量的值</li>
<li>$\sum_{i=1}^m$: 这个是对所有训练数据集求和。从这个也可以看到每迭代一次就要遍历一次全部训练数据集。所以这个算法也称为<strong>批量梯度下降算法 (Batch Gradient Descent) </strong>。对训练数据集比较大的场景下，计算成本是很高的。后面我们会介绍另外一个提高运算效率的算法。</li>
</ul>
<p>这个公式有些符合直觉的地方，比如 $\left(h(x^{(i)}) - y^{(i)}\right)$ 表示的是预测值与真实值的误差，当误差比较大时，经过一轮的迭代，$\theta_j$ 的步幅就迈得比较大。即当我们的参数 $\theta$ 离我们的目标值很远的时候，迭代一次的值变化比较大，可以快速地收敛，而当 $\theta$ 离目标值比较近的时候，迭代一次的值变化比较小，即慢慢地收敛到目标值。</p>
<p>这个公式怎么样用编程语言来实现呢？在编写机器学习算法的时候，一般步骤如下：</p>
<ul>
<li><strong>确定学习率 $\alpha$ </strong><br />
  $\alpha$ 太大可能会使成本函数无法收敛，太小计算太多，机器学习算法效率就比较低。</li>
<li><strong>确定参数起始点</strong><br />
  比如让所有的参数都为 1 作为起点，即 $\theta_0 := 1, \theta_1 := 1, &hellip; \theta_n := 1$。这样就得到了我们的预测函数：$h_\theta(x) = \sum_{i=1}^m x^{(i)}$。根据预测值和我们的成本函数，就可以算出我们在参数起始位置的成本。需要注意的是，参数起始点可以根据实际情况灵活选择，以便让机器学习算法的性能更高，比如选择比较靠近极点的位置。</li>
<li><strong>计算参数的下一组值</strong><br />
  根据 LSM 算法，<strong>分别同时算出</strong>新的 $\theta_j$ 的值。然后用新的 $\theta$ 值得到新的预测函数 $h_\theta(x)$，再根据新的预测函数，代入成本函数就可以算出新的成本。</li>
<li><strong>确认成本函数是否收敛</strong><br />
  拿新的成本和旧的成本进行比较，看成本是不是变得越来越小。如果两次成本之间的差异小于误差范围，即说明我们已经非常靠近最小成本附近了。就可以近似地认为我们找到了最小成本了。如果两次成本之间的差异在误差范围之外，重复步骤 3 继续计算下一组参数 $\theta$。直到找到我们的最优解。</li>
</ul>
<h2 id="_3">随机梯度下降算法</h2>
<p>批量梯度下降算法对参数进行一次迭代运算，就需要遍历所有的训练数据集。当训练数据集比较大时，这个算法的效率会很低。考虑另外一个算法：</p>
<p>$$<br />
\theta_j = \theta_j - \alpha \left(\left(h(x^{(i)}) - y^{(i)}\right) x_j^{(i)}\right)<br />
$$</p>
<p>这就是 <strong>随机梯度下降算法</strong> (stochastic gradient descent)。这个算法的关键点是不去遍历所有的训练数据集，而是改成每次随机地从训练数据集里取一个数据进行参数迭代计算。</p>
<div class="admonition hint">
<p class="admonition-title">怎么理解随机</p>
<p>为什么这么神奇呢？为什么随机从训练数据集里选取一个数据来迭代，不但不影响最终计算结果，还大大地提高了效率。看数学时最怕的就是 <strong>我们考虑 bla bla bla</strong>，作者说出 “我们考虑 bla bla bla” 时背后的过程是怎么样的？坦白讲，怎么样从数学上证明随机梯度下降算法和批量梯度下降算法是等价的，我也不知道。不过我有个直观的可以帮助理解的解释。回到成本函数的定义：$J(\theta) = \frac{1}{2m} \sum_{i=1}^m \left(h(x^{(i)}) - y^{(i)}\right)^2$。我们说过，这里累加后除以 2 是为了计算方便，那么我们除以 m 是什么意思呢？答案是平均值，即所有训练数据集上的点到我们预测函数的距离的<strong>平均值</strong>。再回到<strong>随机选取训练数据集里的一个数据</strong>这个做法来看，如果计算次数足够多，并且是真正随机，那么随机选取出来的这组数据从概率的角度来看，和<strong>平均值</strong>是相当的。打个比方，有一个储钱罐里有 1 角的硬币 10 枚，5 角的硬币 2 枚，1 元的硬币 1 枚，总计 3 元，13 枚硬币。你随机从里面取 1000 次，每次取出来的硬币把币值记录下来，然后放回储钱罐里。这样最后去算这 1000 次取出来的钱的平均值 (1000 次取出来的币值总和除以 1000) 和储钱罐里每枚硬币的平均值 (3/13 元) 应该是近似相等的。</p>
</div>
<p>这样，我们基本上把线性回归算法，最小均方差，随机梯度下降算法的来龙去脉理了一遍。</p>
	<hr/>
	<h6>Post by <a href="./author/joey-huang.html">Joey Huang</a> under <a href="./category/ml.html">ml</a> on 2015-09-03(Thursday) 20:20. Tags: <a href="./tag/machine-learning.html">machine-learning</a>, </h6>
</article>

<hr/>
<div class="row">
	<div class="small-12 columns">
		<h3>Comments</h3>
		<div id="disqus_thread"></div>
		<script type="text/javascript">
			var disqus_shortname = 'kamidox';
			(function() {
				var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
				dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
				(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
			})();
		</script>
		<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
		<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
	</div>
</div>
						</div>
						<!-- End Main Content -->
						<!-- Sidebar -->
						<aside class="medium-3 hide-for-small-only columns">
							<div class="panel">
								<h5>Places</h5>
								<ul class="side-nav">
										<li><a href="http://blog.kamidox.com/feeds/rss.xml" rel="alternate">RSS Feed</a></li>

								</ul>
							</div>


							<div class="panel">
								<h5>Categories</h5>
								<ul class="side-nav">
										<li><a href="./category/android.html">android</a></li>
										<li><a href="./category/essay.html">essay</a></li>
										<li><a href="./category/flask.html">flask</a></li>
										<li><a href="./category/ml.html">ml</a></li>
										<li><a href="./category/nlp.html">nlp</a></li>
										<li><a href="./category/python.html">python</a></li>
										<li><a href="./category/tools.html">tools</a></li>
										<li><a href="./category/weapp.html">weapp</a></li>
										<li><a href="./category/web.html">web</a></li>
										<li><a href="./category/werkzeug.html">werkzeug</a></li>
								</ul>
							</div>

							<div class="panel">
								<h5>Tags</h5>
								<ul class="tag-cloud">
										<li class="tag-4"><a href="./tag/socketserver.html">SocketServer</a></li>
										<li class="tag-2"><a href="./tag/weapp.html">weapp</a></li>
										<li class="tag-2"><a href="./tag/flask.html">flask</a></li>
										<li class="tag-2"><a href="./tag/markdown.html">markdown</a></li>
										<li class="tag-3"><a href="./tag/web.html">web</a></li>
										<li class="tag-2"><a href="./tag/tools.html">tools</a></li>
										<li class="tag-3"><a href="./tag/sublime.html">sublime</a></li>
										<li class="tag-1"><a href="./tag/python.html">python</a></li>
										<li class="tag-4"><a href="./tag/wekzeug.html">wekzeug</a></li>
										<li class="tag-4"><a href="./tag/contacts-provider.html">contacts provider</a></li>
										<li class="tag-2"><a href="./tag/android.html">android</a></li>
										<li class="tag-4"><a href="./tag/patchrom.html">patchrom</a></li>
										<li class="tag-1"><a href="./tag/thought.html">thought</a></li>
										<li class="tag-4"><a href="./tag/nlp.html">nlp</a></li>
										<li class="tag-4"><a href="./tag/miui.html">miui</a></li>
										<li class="tag-4"><a href="./tag/decorator.html">decorator</a></li>
										<li class="tag-1"><a href="./tag/machine-learning.html">machine-learning</a></li>
										<li class="tag-4"><a href="./tag/uml.html">uml</a></li>
										<li class="tag-4"><a href="./tag/contacts.html">contacts</a></li>
										<li class="tag-4"><a href="./tag/ebook.html">ebook</a></li>
										<li class="tag-4"><a href="./tag/react.html">react</a></li>
										<li class="tag-3"><a href="./tag/pelican.html">pelican</a></li>
										<li class="tag-4"><a href="./tag/github.html">github</a></li>
								</ul>
							</div>

							<div class="panel">
								<h5>Monthly Archives</h5>
								<ul class="side-nav">
											<li><a href="/posts/2018/03/index.html">March 2018 (1)</a></li>
											<li><a href="/posts/2017/05/index.html">May 2017 (2)</a></li>
											<li><a href="/posts/2017/04/index.html">April 2017 (1)</a></li>
											<li><a href="/posts/2017/02/index.html">February 2017 (1)</a></li>
											<li><a href="/posts/2017/01/index.html">January 2017 (1)</a></li>
											<li><a href="/posts/2016/12/index.html">December 2016 (2)</a></li>
											<li><a href="/posts/2016/11/index.html">November 2016 (3)</a></li>
											<li><a href="/posts/2016/10/index.html">October 2016 (1)</a></li>
											<li><a href="/posts/2016/09/index.html">September 2016 (1)</a></li>
											<li><a href="/posts/2016/03/index.html">March 2016 (2)</a></li>
											<li><a href="/posts/2016/02/index.html">February 2016 (2)</a></li>
											<li><a href="/posts/2016/01/index.html">January 2016 (2)</a></li>
											<li><a href="/posts/2015/12/index.html">December 2015 (10)</a></li>
											<li><a href="/posts/2015/11/index.html">November 2015 (6)</a></li>
											<li><a href="/posts/2015/10/index.html">October 2015 (2)</a></li>
											<li><a href="/posts/2015/09/index.html">September 2015 (7)</a></li>
											<li><a href="/posts/2015/08/index.html">August 2015 (1)</a></li>
											<li><a href="/posts/2015/07/index.html">July 2015 (1)</a></li>
											<li><a href="/posts/2015/05/index.html">May 2015 (1)</a></li>
											<li><a href="/posts/2015/04/index.html">April 2015 (1)</a></li>
											<li><a href="/posts/2015/03/index.html">March 2015 (3)</a></li>
											<li><a href="/posts/2015/02/index.html">February 2015 (2)</a></li>
											<li><a href="/posts/2015/01/index.html">January 2015 (2)</a></li>
											<li><a href="/posts/2014/12/index.html">December 2014 (3)</a></li>
											<li><a href="/posts/2014/11/index.html">November 2014 (4)</a></li>
											<li><a href="/posts/2014/10/index.html">October 2014 (6)</a></li>
											<li><a href="/posts/2014/09/index.html">September 2014 (1)</a></li>
											<li><a href="/posts/2014/07/index.html">July 2014 (1)</a></li>
								</ul>
							</div>

						</aside>
						<!-- End Sidebar -->
					</div>

					<!-- Footer -->
					<footer class="row">
						<div class="medium-9 small-12">
							<hr/>
							<p class="text-center">Powered by <a href="http://getpelican.com">Pelican</a> and <a href="http://foundation.zurb.com/">Zurb Foundation</a>. Theme by <a href="http://hamaluik.com">Kenton Hamaluik</a>.
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1253471695'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "v1.cnzz.com/z_stat.php%3Fid%3D1253471695%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
							</p>
						</div>
					</footer>
					<!-- End Footer -->
				</section>
				<a class="exit-off-canvas"></a>
			</div><!--off-canvas inner-->
		</div><!--off-canvas wrap-->

		<script src="./theme/js/jquery.js"></script>
		<script src="./theme/js/foundation.min.js"></script>
		<script>
			$(document).foundation();
		</script>
	</body>
</html>